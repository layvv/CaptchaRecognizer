from collections import defaultdict

import torch
import torch.nn.functional as F
from torch import nn
from torch.utils.data import DataLoader
from tqdm import tqdm

from model.char.config import BaseConfig
from model.char.utils.file_util import create_experiment_dir, save_final_model, save_checkpoint, log_startup_info, \
    save_manager
from model.char.utils.visualizer import Visualizer


class BasicBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        super().__init__()
        self.conv1 = nn.Conv2d(
            in_channels, out_channels, kernel_size=3,
            stride=stride, padding=1, bias=False
        )
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = nn.Conv2d(
            out_channels, out_channels, kernel_size=3,
            stride=1, padding=1, bias=False
        )
        self.bn2 = nn.BatchNorm2d(out_channels)
        self.shortcut = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(out_channels)
            )

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out += self.shortcut(x)
        return F.relu(out)


class ResNetMultiHead(nn.Module):
    name = 'resnet_multi_head'
    num_classes = BaseConfig.NUM_CLASSES
    captcha_length = BaseConfig.CAPTCHA_LENGTH
    batch_size = 128
    epochs = 150
    lr = 1e-3
    min_lr = 1e-6
    num_workers = 4
    pin_memory = True
    persistent_workers = True
    early_stop_patience = 10
    early_stop_delta = 0.002
    weight_decay = 0.05
    conv_dropout = 0.2
    shared_dropout = 0.3
    head_dropout = 0.2


    def __init__(self):
        super().__init__()

        # é‡æ„ç‰¹å¾æå–å±‚
        self.stem = nn.Sequential(
            nn.Conv2d(1, 32, 3, stride=1, padding=1, bias=False),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        )

        # æ„å»ºæ®‹å·®å—åºåˆ—
        self.layer1 = self._make_layer(block=BasicBlock, in_channels=32, out_channels=64, blocks=2, stride=1)
        self.layer2 = self._make_layer(block=BasicBlock, in_channels=64, out_channels=128, blocks=2, stride=2)
        self.layer3 = self._make_layer(block=BasicBlock, in_channels=128, out_channels=256, blocks=2, stride=2)

        # æ·»åŠ SEæ³¨æ„åŠ›æ¨¡å—
        self.se = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Conv2d(256, 64, kernel_size=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 256, kernel_size=1),
            nn.Sigmoid()
        )

        # ä¿®æ”¹åˆ†ç±»å¤´å‰çš„æ± åŒ–å±‚
        self.global_pool = nn.AdaptiveAvgPool2d(1)  # è‡ªé€‚åº”æ± åŒ–

        self.shared_fc = nn.Sequential(
            nn.Linear(256, 512),  # ç›´æ¥ä½¿ç”¨SEæ¨¡å—çš„è¾“å‡ºé€šé“æ•°
            nn.BatchNorm1d(512),
            nn.ReLU(),
            nn.Dropout(self.shared_dropout)
        )

        # å¤šä»»åŠ¡å¤´
        self.heads = nn.ModuleList([
            nn.Sequential(
                nn.Linear(512, 256),
                nn.BatchNorm1d(256),
                nn.ReLU(),
                nn.Dropout(self.head_dropout),
                nn.Linear(256, self.num_classes)
            ) for _ in range(self.captcha_length)
        ])

        self._init_weights()  # æƒé‡åˆå§‹åŒ–

    @staticmethod
    def _make_layer(block, in_channels, out_channels, blocks, stride=1):
        layers = [block(in_channels, out_channels, stride)]
        for _ in range(1, blocks):
            layers.append(block(out_channels, out_channels))
        return nn.Sequential(*layers)

    def _init_weights(self):
        # åªåˆå§‹åŒ–æœªåŠ è½½çš„å‚æ•°
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
        # å¤´éƒ¨ç½‘ç»œç‰¹æ®Šåˆå§‹åŒ–ï¼ˆä¼šè¢«é¢„è®­ç»ƒå‚æ•°è¦†ç›–ï¼‰
        for head in self.heads:
            nn.init.normal_(head[-1].weight, mean=0, std=0.01)

    def _init_training_config(self):
        # åˆå§‹åŒ–è®¾å¤‡
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.to(self.device)  # å°†æ¨¡å‹ç§»åŠ¨åˆ°è®¾å¤‡

        # ä¼˜åŒ–å™¨
        self.optimizer = torch.optim.AdamW(
            self.parameters(),
            lr=self.lr,
            weight_decay=self.weight_decay
        )
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer,
            mode='max',
            factor=0.3,
            patience=5,
            min_lr=self.min_lr
        )

        # æŸå¤±å‡½æ•°
        self.criterion = nn.CrossEntropyLoss(label_smoothing=0.1)

    def _init_training_state(self):
        """è®­ç»ƒçŠ¶æ€åˆå§‹åŒ–"""
        self.best_val_loss = float('inf')
        self.best_val_acc = 0.0
        self.train_losses = []
        self.val_losses = []
        self.train_accs = []
        self.val_accs = []
        self.learning_rates = []
        self.no_improve_counter = 0
        self.is_early_stop = False
        # éªŒè¯é›†å­—ç¬¦ç»Ÿè®¡
        self.val_char_distribution = defaultdict(lambda: {'total': 0, 'correct': 0})
        # å®éªŒç›®å½•
        self.experiment_dir = create_experiment_dir(
            model_name=self.name,
            model_params={
                'batch_size': self.batch_size,
                'lr': self.lr,
            }
        )
        self.visualizer = Visualizer(self.experiment_dir)

    def _load_data(self, num_samples=None):

        from model.char.data.dataset import CaptchaDataset

        # åŠ è½½æ•°æ®é›†
        self.train_dataset = CaptchaDataset(num_samples, 'train')
        self.val_dataset = CaptchaDataset(num_samples, 'valid')

        # æ„å»ºæ•°æ®ä¿¡æ¯
        data_info = {
            "è®­ç»ƒé›†æ ·æœ¬": len(self.train_dataset),
            "éªŒè¯é›†æ ·æœ¬": len(self.val_dataset),
            "å›¾åƒå°ºå¯¸": BaseConfig.IMAGE_SIZE,
            "å­—ç¬¦é•¿åº¦": BaseConfig.CAPTCHA_LENGTH,
            "å­—ç¬¦ç±»åˆ«æ•°": BaseConfig.NUM_CLASSES
        }

        # æ‰“å°ç¾è§‚çš„æ—¥å¿—
        print("\nâ•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Data Loading â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®")
        for i, (k, v) in enumerate(data_info.items()):
            prefix = "â”‚ " if i == 0 else "â”‚ "
            print(f"{prefix}{k + ':':<14} {v}")
        print("â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯\n")

        # æ•°æ®åŠ è½½å™¨
        self.train_loader = DataLoader(
            self.train_dataset,
            batch_size=self.batch_size,
            shuffle=True,
            num_workers=self.num_workers,
            pin_memory=self.pin_memory,
            persistent_workers=self.persistent_workers if self.num_workers > 0 else False,
        )

        self.valid_loader = DataLoader(
            self.val_dataset,
            batch_size=self.batch_size,
            shuffle=False,
            num_workers=self.num_workers,
            pin_memory=self.pin_memory,
            persistent_workers=self.persistent_workers if self.num_workers > 0 else False,
        )

        # åœ¨åŠ è½½éªŒè¯é›†åç»Ÿè®¡æ€»å­—ç¬¦æ•°ï¼ˆåªæ‰§è¡Œä¸€æ¬¡ï¼‰
        for _, labels in tqdm(DataLoader(self.val_dataset, batch_size=self.batch_size), 
                            desc="ç»Ÿè®¡éªŒè¯é›†å­—ç¬¦", total=len(self.val_dataset)//self.batch_size):
            for i in range(labels.size(0)):
                for j in range(labels.size(1)):
                    char_idx = labels[i][j].item()
                    char = BaseConfig.CHAR_SET[char_idx]
                    self.val_char_distribution[char]['total'] += 1

    def _forward_features(self, x):
        x = self.stem(x)
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.se(x) * x  # SEæ³¨æ„åŠ›
        x = self.global_pool(x)
        return x.view(x.size(0), -1)

    def forward(self, x):
        # ç‰¹å¾æå–
        x = self._forward_features(x)
        # å±•å¹³å¤„ç†
        x = torch.flatten(x, 1)
        shared = self.shared_fc(x)
        return [head(shared) for head in self.heads]

    def _eval(self, epoch):
        self.eval()
        total_loss = 0
        total_acc = 0
        all_labels = []
        all_preds = []
        # å¹³å‡æ¯ä¸ªä½ç½®çš„æ­£ç¡®ç‡
        position_acc = [0.0] * self.captcha_length
        # å­—ç¬¦åˆ†å¸ƒç»Ÿè®¡ï¼ˆåªç»Ÿè®¡æ­£ç¡®æ•°ï¼‰
        char_distribution = defaultdict(lambda: {'correct': 0, 'total': 0})
        # åˆå§‹åŒ–æ—¶ä½¿ç”¨é¢„ç»Ÿè®¡çš„æ€»æ•°
        for char, stats in self.val_char_distribution.items():
            char_distribution[char]['total'] = stats['total']

        with torch.no_grad():
            desc = f'valid Epoch {epoch}/{self.epochs}'
            val_bar = tqdm(
                self.valid_loader,
                desc=desc,
                leave=False
            )
            for images, labels in val_bar:
                images = images.to(self.device)
                labels = labels.to(self.device)
                outputs = self(images)
                loss = sum(self.criterion(output, labels[:, i]) for i, output in enumerate(outputs))

                full_acc, preds = self._calculate_accuracy(outputs, labels)
                total_loss += loss.item()
                total_acc += full_acc

                val_bar.set_postfix({
                    'loss': f'{loss.item():.4f}',
                    'acc': f'{full_acc * 100:.2f}%'
                })

                all_preds.extend(preds.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())

                # ç»Ÿè®¡å­—ç¬¦åˆ†å¸ƒï¼ˆåªæ›´æ–°æ­£ç¡®æ•°ï¼‰
                for i in range(labels.size(0)):
                    for j in range(labels.size(1)):
                        char_idx = labels[i][j].item()
                        char = BaseConfig.CHAR_SET[char_idx]
                        if preds[i][j] == labels[i][j]:
                            char_distribution[char]['correct'] += 1

                # ç»Ÿè®¡æ¯ä¸ªä½ç½®çš„å‡†ç¡®ç‡
                pos_correct = [0] * self.captcha_length
                for pos in range(self.captcha_length):
                    pos_correct[pos] += (preds[:, pos] == labels[:, pos]).sum().item()
                    position_acc[pos] = pos_correct[pos] / labels.size(0)

            # è®°å½•æ··æ·†çŸ©é˜µ
            self.visualizer.log_confusion_matrix(all_labels, all_preds, epoch)

            # è®°å½•ä½ç½®æ­£ç¡®ç‡
            self.visualizer.log_char_pos_acc(position_acc, epoch)

            # è®°å½•å­—ç¬¦ç»Ÿè®¡
            self.visualizer.log_char_distribution(char_distribution, epoch)

        return total_loss / len(self.valid_loader), total_acc / len(self.valid_loader)

    def _train(self, epoch):

        if torch.cuda.is_available():
            torch.cuda.reset_peak_memory_stats()

        self.train()
        total_loss = 0
        total_acc = 0
        progress_bar = tqdm(
            self.train_loader,
            desc=f'Train Epoch {epoch}/{self.epochs}',
            leave=False
        )
        for _, (images, labels) in enumerate(progress_bar):
            images = images.to(self.device)
            labels = labels.to(self.device)

            # å‰å‘ä¼ æ’­
            outputs = self(images)

            # è®¡ç®—å¤šä»»åŠ¡æŸå¤±
            loss = sum(self.criterion(output, labels[:, i]) for i, output in enumerate(outputs))

            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=2.0)
            self.optimizer.step()

            # è®¡ç®—æŒ‡æ ‡
            total_loss += loss.item()
            full_acc, _ = self._calculate_accuracy(outputs, labels)
            total_acc += full_acc

            # å®æ—¶æ›´æ–°è¿›åº¦ä¿¡æ¯
            progress_bar.set_postfix({
                'loss': f'{loss.item():.4f}',
                'acc': f'{full_acc * 100:.2f}%',
                'lr': f'{self.optimizer.param_groups[0]["lr"]:.2e}'
            })

        return total_loss / len(self.train_loader), total_acc / len(self.train_loader)

    @staticmethod
    def _calculate_accuracy(outputs, labels):
        with torch.no_grad():
            # å®Œå…¨æ­£ç¡®ç‡
            preds = torch.stack([output.argmax(1) for output in outputs], dim=1)
            all_correct = (preds == labels).all(dim=1).sum().item()

            return all_correct / labels.size(0), preds

    def _check_early_stop(self, val_loss, epoch):
        # æ—©åœæœºåˆ¶
        if val_loss < self.best_val_loss - self.early_stop_delta:
            self.best_val_loss = val_loss
            self.no_improve_counter = 0
        else:
            self.no_improve_counter += 1
            if self.no_improve_counter >= self.early_stop_patience:
                print(f'Early stopping after {epoch} epochs')
                self.is_early_stop = True

    def start(self, num_samples=None):
        """è®­ç»ƒå…¥å£æ–¹æ³•"""
        self._init_training_config()

        self._init_training_state()

        self._load_data(num_samples)

        log_startup_info(self)

        for epoch in range(1, self.epochs + 1):

            # ä¸€æ¬¡epochå†…, è®­ç»ƒé›†çš„å¹³å‡æŸå¤±å’Œå‡†ç¡®ç‡
            train_loss, train_acc = self._train(epoch)

            # éªŒè¯é˜¶æ®µï¼Œè¿”å›ä¸€æ¬¡epochå†…, éªŒè¯é›†çš„å¹³å‡æŸå¤±å’Œå‡†ç¡®ç‡
            val_loss, val_acc = self._eval(epoch)
            self.scheduler.step(val_acc)
            current_lr = self.optimizer.param_groups[0]['lr']

            self.train_losses.append(train_loss)
            self.train_accs.append(train_acc)
            self.val_losses.append(val_loss)
            self.val_accs.append(val_acc)
            self.learning_rates.append(current_lr)
            self.best_val_acc = max(self.best_val_acc, val_acc)

            # è®°å½•æ ‡é‡æ•°æ®
            self.visualizer.log_scalars('Loss', {
                'train': train_loss,
                'valid': val_loss
            }, epoch)

            self.visualizer.log_scalars('Accuracy', {
                'train': train_acc,
                'valid': val_acc
            }, epoch)

            self.visualizer.log_learning_rate(current_lr, epoch)

            # è®°å½•æƒé‡åˆ†å¸ƒ
            for name, param in self.named_parameters():
                self.visualizer.log_histogram(name, param, epoch)

            save_checkpoint(self, epoch)

            self._check_early_stop(val_loss, epoch)

            if self.is_early_stop:
                break

            # æ‰“å°è®­ç»ƒä¿¡æ¯
            print(f'Epoch {epoch:02d} | '
                  f'Train Loss/Acc: {train_loss:.4f} / {train_acc * 100:.2f}% | '
                  f'Valid Loss/Acc: {val_loss:.4f} / {val_acc * 100:.2f}% | '
                  f'LR: {self.optimizer.param_groups[0]["lr"]:.2e} | '
                  f'No Improve: {self.no_improve_counter} | '
                  f'Best Loss: {self.best_val_loss:.2f} | '
                  f'Best Acc: {self.best_val_acc * 100:.2f}%'
                  )
        save_final_model(self)
        
        # åœ¨è®­ç»ƒç»“æŸåæ·»åŠ 
        save_manager.shutdown()  # ç¡®ä¿æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        print(f"ğŸ è®­ç»ƒå®Œæˆï¼")
