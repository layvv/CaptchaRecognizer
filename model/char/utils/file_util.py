import os
import shutil
import threading
from datetime import datetime
from queue import Queue

import psutil
import torch
from PIL import ImageFont

from model.char.config import CheckpointConfig, BaseConfig


def clear_dir(path):
    # æ¸…ç©ºç›®å½•å‰éªŒè¯
    if os.path.exists(path):
        print(f"â™»ï¸ æ¸…ç©ºå·²æœ‰æ•°æ®ï¼š{path} (å…±{len(os.listdir(path))}ä¸ªæ—§æ ·æœ¬)")
        shutil.rmtree(path)  # åˆ é™¤æ•´ä¸ªç›®å½•æ ‘
        os.mkdir(path)  # é‡æ–°åˆ›å»ºç›®å½•
    else:
        print(f"ğŸ“ ç›®å½•ä¸å­˜åœ¨ï¼Œå·²é‡æ–°åˆ›å»º: {path}")
        os.mkdir(path)  # é‡æ–°åˆ›å»ºç›®å½•


def load_fonts(font_dir):
    """åŠ è½½å¹¶éªŒè¯å­—ä½“æ–‡ä»¶"""
    valid_fonts = []
    for filename in os.listdir(font_dir):
        font_path = os.path.join(font_dir, filename)
        try:
            # éªŒè¯å­—ä½“æœ‰æ•ˆæ€§
            font = ImageFont.truetype(font_path, size=20)
            valid_fonts.append(font_path)
        except Exception as e:
            print(f"âš ï¸ è·³è¿‡æ— æ•ˆå­—ä½“: {filename} ({str(e)})")
    return valid_fonts


def create_experiment_dir(model_name, model_params):
    """åˆ›å»ºå®éªŒç›®å½•"""
    params = {
        'timestamp': datetime.now().strftime("%Y-%m-%d_%H-%M"),
        'model_name': model_name,
        'batch_size': model_params['batch_size'],
        'lr': model_params['lr'],
    }
    dir_name = CheckpointConfig.EXPERIMENT_FORMAT.format(**params)
    exp_dir = os.path.join(CheckpointConfig.CHECKPOINT_ROOT, dir_name)
    os.makedirs(exp_dir, exist_ok=True)
    return exp_dir


# å…¨å±€ä¿å­˜ç®¡ç†ç±»
class SaveManager:
    def __init__(self):
        self.save_queue = Queue()
        self.save_thread = None
        self.lock = threading.Lock()
        self.running = True  # æ–°å¢è¿è¡ŒçŠ¶æ€æ ‡å¿—

    def add_task(self, task):
        with self.lock:
            # ç¡®ä¿çº¿ç¨‹æŒç»­è¿è¡Œ
            if not self.save_thread or not self.save_thread.is_alive():
                self.save_thread = threading.Thread(target=self._process_queue, daemon=True)
                self.save_thread.start()
            self.save_queue.put(task)

    def _process_queue(self):
        while self.running or not self.save_queue.empty():  # ä¿®æ”¹å¾ªç¯æ¡ä»¶
            try:
                task = self.save_queue.get()
                task()
            except Exception as e:
                print(f"âŒ ä¿å­˜ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {str(e)}")
            finally:
                self.save_queue.task_done()

    def shutdown(self):
        self.running = False
        if self.save_thread:
            self.save_thread.join()

save_manager = SaveManager()

def save_checkpoint(model, epoch):
    """ä¿å­˜å‡†ç¡®ç‡æ›´é«˜çš„æ¨¡å‹"""
    if model.val_accs[-1] < model.best_val_acc:
        return
    
    # å°†ä¿å­˜ä»»åŠ¡åŠ å…¥é˜Ÿåˆ—
    save_manager.add_task(lambda: _save_checkpoint(model, epoch))

def save_final_model(model):
    """å°†æœ€ç»ˆæ¨¡å‹ä¿å­˜ä»»åŠ¡åŠ å…¥é˜Ÿåˆ—"""
    save_manager.add_task(lambda: _do_final_save(model))

def _do_final_save(model):
    model_path = None
    for file in os.listdir(model.experiment_dir):
        if file.endswith('.pth'):
            model_path = os.path.join(model.experiment_dir, file)
            break
    if not model_path:
        print("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•æ£€æŸ¥ç‚¹æ–‡ä»¶ï¼")  # è°ƒè¯•æ—¥å¿—
        return
    state = torch.load(model_path, map_location=torch.device("cuda" if torch.cuda.is_available() else "cpu"))
    # æ¸…ç†ä¸éœ€è¦çš„é”®
    for key in ['optimizer_state_dict', 'scheduler_state_dict', 'epoch']:
        if key in state:
            del state[key]
    
    final_model_path = os.path.join(CheckpointConfig.FINAL_DIR, f'{model.name}.pth')
    torch.save(state, final_model_path)
    print(f"ğŸ’¾ æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: {final_model_path}")  # è°ƒè¯•æ—¥å¿—

def _save_checkpoint(model, epoch):
    # ç§»é™¤æ—§æ¨¡å‹
    model_files = [f for f in os.listdir(model.experiment_dir) if f.endswith('.pth')]
    for file in model_files:
        os.remove(os.path.join(model.experiment_dir, file))

    # ä¿å­˜æ–°æ£€æŸ¥ç‚¹
    checkpoint_path = os.path.join(
        model.experiment_dir,
        f'{model.name}_epoch{epoch}_acc{model.best_val_acc * 100:.2f}.pth'
    )
    
    state = {
        'model_class' : model.__class__.__name__,
        'model_module': model.__class__.__module__,
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': model.optimizer.state_dict(),
        'scheduler_state_dict': model.scheduler.state_dict(),
    }
    torch.save(state, checkpoint_path)
    
    # ä¿å­˜é…ç½®æ–‡ä»¶
    save_training_config(model)


def save_training_config(model):
    """ç‹¬ç«‹ä¿å­˜é…ç½®çš„æ–¹æ³•"""
    config_path = os.path.join(model.experiment_dir, 'training_config.json')
    config = {
        'model': {
            'name': model.name,
            'class': model.__class__.__name__,
            'conv_dropout': model.conv_dropout,
            'shared_dropout': model.shared_dropout,
            'head_dropout': model.head_dropout,
            'captcha_length': model.captcha_length,
            'num_classes': model.num_classes,
        },
        'training': {
            'batch_size': model.batch_size,
            'epochs': model.epochs,
            'lr': model.lr,
            'weight_decay': model.weight_decay,
            'early_stop_patience': model.early_stop_patience,
            'early_stop_delta': model.early_stop_delta,
            'best_val_loss': model.best_val_loss,
            'best_val_acc': model.best_val_acc,
            'total_epochs': len(model.train_losses),
            'valid_accs': model.val_accs,
            'valid_losses': model.val_losses,
        },
        'dataset': {
            'IMAGE_SIZE': BaseConfig.IMAGE_SIZE,
            'CHAR_SET': BaseConfig.CHAR_SET,
            'CAPTCHA_LENGTH': BaseConfig.CAPTCHA_LENGTH,
            'NUM_CLASSES': BaseConfig.NUM_CLASSES
        },
        'environment': {
            'saved_time': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            'device': str(model.device),
            'torch_version': torch.__version__
        }
    }

    import json
    with open(config_path, 'w') as f:
        json.dump(config, f, indent=2, ensure_ascii=False)


def log_startup_info(model):
    """ä¼˜åŒ–åçš„è®­ç»ƒå¯åŠ¨ä¿¡æ¯ï¼ˆèšç„¦æ ¸å¿ƒå‚æ•°ï¼‰"""
    # æ ¸å¿ƒä¿¡æ¯åˆ†ç±»
    env_info = {
        "PyTorch Ver": torch.__version__,
        "CUDA Available": "âœ…" if torch.cuda.is_available() else "âŒ",
        "GPU Name": torch.cuda.get_device_name(0) if torch.cuda.is_available() else "N/A",
    }

    hardware_info = {
        "CPU Cores": os.cpu_count(),
        "RAM": f"{psutil.virtual_memory().total / 1024 ** 3:.1f}G",
    }

    training_config = {
        "Batch Size": model.batch_size,
        "Init LR": model.lr,
        "Weight Decay": model.weight_decay,
        "Max Epochs": model.epochs,
        "Early Stop": f"{model.early_stop_patience} epochs (Î”<{model.early_stop_delta})",
        "Tracking Positions": model.captcha_length
    }

    dataset_info = {
        "Image Size": BaseConfig.IMAGE_SIZE,
        "Char Length": BaseConfig.CAPTCHA_LENGTH,
        "Char Classes": BaseConfig.NUM_CLASSES,
        "Train Samples": len(model.train_dataset),
        "Valid Samples": len(model.val_dataset)
    }

    model_info = {
        "Backbone": "ResNet",
        "Attention": "SE Block",
        "Total Params": f"{sum(p.numel() for p in model.parameters()) / 1e6:.2f}M"
    }

    # ä¿¡æ¯æ’ç‰ˆ
    def format_section(title, items, width=40):
        lines = [f"â•â• {title} â•" + "â•" * (width - len(title) - 4)]
        for k, v in items.items():
            line = f"â”‚ {k:<16} {v}"
            lines.append(line.ljust(width - 1) + "â”‚")
        return "\n".join(lines)

    # æ„å»ºæ˜¾ç¤ºå†…å®¹
    content = [
        format_section("Environment", env_info),
        format_section("Hardware", hardware_info),
        format_section("Training Config", training_config),
        format_section("Dataset Info", dataset_info),
        format_section("Model Architecture", model_info),
        f"â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯",
        f"ğŸ“ Experiment Dir: {model.experiment_dir}"
    ]

    # æ‰“å°ä¿¡æ¯
    print("\nâ•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Training Session â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®")
    print("\n".join(content))
