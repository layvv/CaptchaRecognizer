import torch
import torchvision
from torch import nn
from torch.utils.data import DataLoader
from tqdm import tqdm

from model.char.config import BaseConfig
from model.char.data.dataset import CaptchaDataset
from model.char.utils import create_experiment_dir, save_checkpoint


class ResNet18MultiHead(nn.Module):
    name = 'resnet18_multi_head'
    batch_size = 64
    epochs = 50
    lr = 1e-3
    num_workers = 4
    pin_memory = True
    persistent_workers = True
    early_stop_patience = 10
    early_stop_delta = 0.001
    dropout = 0.5
    save_interval = 5  # æ¯5ä¸ªepochä¿å­˜ä¸€æ¬¡

    def __init__(self):
        super().__init__()
        self.num_classes = BaseConfig.NUM_CLASSES
        self.captcha_length = BaseConfig.CAPTCHA_LENGTH

        # ä½¿ç”¨é…ç½®åˆå§‹åŒ–ç½‘ç»œ
        self.backbone = torchvision.models.resnet18(weights='DEFAULT')
        in_features = self.backbone.fc.in_features
        self.backbone.fc = nn.Identity()

        self.heads = nn.ModuleList([
            nn.Sequential(
                nn.Dropout(self.dropout),
                nn.Linear(in_features, self.num_classes)
            ) for _ in range(self.captcha_length)
        ])

        # ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨
        self.optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr)
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, mode='min', factor=0.1, patience=5
        )
        self.criterion = nn.CrossEntropyLoss()

        # è®­ç»ƒè®¾å¤‡é…ç½®
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.to(self.device)

        self.learning_rates = []
        self.confusion_matrix = None
        self.best_val_loss = float('inf')
        self.train_losses = []
        self.train_accs = []
        self.val_losses = []
        self.val_accs = []
        self.is_early_stop = False
        self.no_improve_counter = 0


    def forward(self, x):
        features = self.backbone(x)
        return [head(features) for head in self.heads]

    @staticmethod
    def _calculate_accuracy(outputs, labels):
        with torch.no_grad():
            total_correct = 0
            # outputs[i]:è¡¨ç¤ºä¸€ä¸ªæ‰¹æ¬¡ä¸­æ‰€æœ‰å›¾ç‰‡åœ¨ç¬¬iä¸ªä½ç½®ä¸Šçš„æ¦‚ç‡åˆ†å¸ƒ
            # ä¾‹å¦‚ä¸€ä¸ªæ‰¹æ¬¡ä¸­è®­ç»ƒ32å¼ 4ä½å­—ç¬¦çš„å›¾ç‰‡ï¼Œé‚£ä¹ˆoutputså…±4ä¸ªoutput,ç¬¬1ä¸ªoutput
            # å­˜å‚¨äº†æ¨¡å‹å¯¹æ¯å¼ å›¾ç‰‡åœ¨ç¬¬1ä¸ªå­—ç¬¦ä¸Šçš„é¢„æµ‹ï¼Œæ¯”å¦‚å¯¹ç¬¬ä¸€å¼ å›¾ç‰‡é¢„æµ‹ï¼š[0.8,0.2,...,0.4]å…±62ä¸ªæ¦‚ç‡å€¼
            # å¯¹åº”äº†'012...Z'62ä¸ªå­—ç¬¦ï¼Œå‡è®¾æ¦‚ç‡æœ€å¤§çš„å€¼ä¸º0.8ï¼Œé‚£ä¹ˆæ¨¡å‹å¯¹è¿™å¼ å›¾ç‰‡ç¬¬ä¸€ä¸ªå­—ç¬¦çš„é¢„æµ‹ä¸º0
            for i, output in enumerate(outputs):
                # iä»£è¡¨å­—ç¬¦ä½ç½®ï¼Œç¬¬iæ¬¡å¾ªç¯ï¼Œpredictedä¿å­˜äº†æ¯å¼ å›¾ç‰‡åœ¨ç¬¬iä¸ªä½ç½®ä¸Šçš„é¢„æµ‹ç»“æœï¼Œç”¨ç´¢å¼•å€¼è¡¨ç¤º
                # æ¯”å¦‚ç¬¬ä¸€æ¬¡å¾ªç¯ï¼Œpredicted=[0,60,...,30]å…±32ä¸ªç´¢å¼•å€¼ï¼Œå¯¹åº”æ¯å¼ å›¾ç‰‡åœ¨ç¬¬1ä¸ªä½ç½®ä¸Šçš„å­—ç¬¦çš„ç´¢å¼•
                _, predicted = output.max(1)
                # labels[:,i]è¡¨ç¤ºå–æ¯å¼ å›¾ç‰‡åœ¨ç¬¬iä¸ªä½ç½®ä¸Šçš„çœŸå®å­—ç¬¦ç´¢å¼•ï¼Œ
                # labels = [
                #   [0,62,1,61],
                #   ...
                #   [30,15,22,10]
                # ]å…±32ä¸ªå…ƒç´ 
                total_correct += (predicted == labels[:, i]).sum().item()
            return total_correct / (labels.size(0) * labels.size(1))

    def start_train(self):
        # åˆ›å»ºå®éªŒç›®å½•ï¼ˆä»…åœ¨è®­ç»ƒæ—¶æ‰§è¡Œï¼‰
        self.experiment_dir = create_experiment_dir(
            model_name=self.name,
            model_params={
                'batch_size': self.batch_size,
                'lr': self.lr
            }
        )

        # å‡†å¤‡æ•°æ®é›†
        train_dataset = CaptchaDataset('train')
        val_dataset = CaptchaDataset('val')

        train_loader = DataLoader(
            train_dataset,
            batch_size=self.batch_size,
            shuffle=True,
            num_workers=self.num_workers,
            pin_memory=self.pin_memory,  # å¯ç”¨å†…å­˜é”é¡µ
            persistent_workers=self.persistent_workers if self.num_workers > 0 else False
        )

        val_loader = DataLoader(
            val_dataset,
            batch_size=self.batch_size,
            shuffle=False,
            num_workers=self.num_workers
        )

        for epoch in range(self.epochs):
            self.train()
            epoch_train_loss = 0
            epoch_train_acc = 0
            progress_bar = tqdm(
                train_loader,
                desc=f'Train Epoch {epoch+1}/{self.epochs}',
                leave=False
            )

            for _, (images, labels) in enumerate(progress_bar):
                images = images.to(self.device)
                labels = labels.to(self.device)

                # å‰å‘ä¼ æ’­
                outputs = self(images)


                # è®¡ç®—å¤šä»»åŠ¡æŸå¤±
                loss = sum(self.criterion(output, labels[:, i]) for i, output in enumerate(outputs))

                # åå‘ä¼ æ’­
                self.optimizer.zero_grad()
                loss.backward()
                self.optimizer.step()

                # è®¡ç®—æŒ‡æ ‡
                epoch_train_loss += loss.item()
                batch_acc = self._calculate_accuracy(outputs, labels)
                epoch_train_acc += batch_acc

                # å®æ—¶æ›´æ–°è¿›åº¦ä¿¡æ¯
                progress_bar.set_postfix({
                    'loss': f'{loss.item():.4f}',
                    'acc': f'{batch_acc*100:.2f}%',
                    'lr': f'{self.optimizer.param_groups[0]["lr"]:.2e}'
                })

            # éªŒè¯é˜¶æ®µï¼Œè¿”å›ä¸€æ¬¡epochå†…, éªŒè¯é›†çš„å¹³å‡æŸå¤±å’Œå‡†ç¡®ç‡
            val_loss, val_acc = self._evaluate(val_loader, epoch + 1)

            # ä¸€æ¬¡epochå†…, è®­ç»ƒé›†çš„å¹³å‡æŸå¤±å’Œå‡†ç¡®ç‡
            train_loss = epoch_train_loss / len(train_loader)
            train_acc = epoch_train_acc / len(train_loader)
            self.train_losses.append(train_loss)
            self.train_accs.append(train_acc)
            self.val_losses.append(val_loss)
            self.val_accs.append(val_acc)

            # æ›´æ–°å­¦ä¹ ç‡
            self.scheduler.step(val_loss)

            current_lr = self.optimizer.param_groups[0]['lr']
            # åœ¨è®­ç»ƒå¾ªç¯ä¸­æ·»åŠ å­¦ä¹ ç‡è®°å½•
            self.learning_rates.append(current_lr)

            # æ—©åœæœºåˆ¶
            if val_loss < self.best_val_loss - self.early_stop_delta:
                self.best_val_loss = val_loss
                self.no_improve_counter = 0
            else:
                self.no_improve_counter += 1
                if self.no_improve_counter >= self.early_stop_patience:
                    # è·³å‡ºå¾ªç¯åä¸ä¼š+1ï¼Œæ‰€ä»¥éœ€è¦æ‰‹åŠ¨+1
                    epoch += 1
                    print(f'Early stopping after {epoch} epochs')
                    self.is_early_stop = True
                    break

            # æ‰“å°è®­ç»ƒä¿¡æ¯
            print(f'Epoch {epoch+1:02d} | '
                  f'Train Loss: {train_loss:.4f} | '
                  f'Train Acc: {train_acc*100:.2f}% | '
                  f'Val Loss: {val_loss:.4f} | '
                  f'Val Acc: {val_acc*100:.2f}% | '
                  f'LR: {self.optimizer.param_groups[0]["lr"]:.2e} | '
                  f'No Improve: {self.no_improve_counter} | '
                  f'Best Val Loss: {self.best_val_loss:.2f} | '
                  )
            self._save_checkpoint(epoch+1)
        print(f"ğŸ è®­ç»ƒå®Œæˆï¼æ¨¡å‹ä¿å­˜åœ¨: {self.experiment_dir}")

    def _evaluate(self, data_loader, epoch_num=None):
        self.eval()
        total_loss = 0
        total_acc = 0

        with torch.no_grad():
            desc = f'Valid Epoch {epoch_num}/{self.epochs}'
            val_bar = tqdm(
                data_loader,
                desc=desc,
                leave=False
            )
            for images, labels in val_bar:
                images = images.to(self.device)
                labels = labels.to(self.device)
                outputs = self(images)
                loss = sum(self.criterion(output, labels[:, i]) for i, output in enumerate(outputs))

                # å®æ—¶æŒ‡æ ‡è®¡ç®—
                batch_acc = self._calculate_accuracy(outputs, labels)
                total_loss += loss.item()
                total_acc += batch_acc

                # ç»Ÿä¸€æŒ‡æ ‡æ˜¾ç¤ºæ ¼å¼
                val_bar.set_postfix({
                    'loss': f'{loss.item():.4f}',
                    'acc': f'{batch_acc*100:.2f}%'
                })

        return total_loss / len(data_loader), total_acc / len(data_loader)


    def _save_checkpoint(self,epoch):
        # å¦‚æœæ¨¡å‹æ—©åœï¼Œåˆ™ä¸ä¿å­˜æœ€åä¸€æ¬¡æ£€æŸ¥ç‚¹
        if self.is_early_stop:
            return
        # å¦‚æœè¾¾åˆ°ä¿å­˜é—´éš”ï¼Œæˆ–è€…è¾¾åˆ°æ€»è½®æ¬¡ï¼Œåˆ™ä¿å­˜æ£€æŸ¥ç‚¹
        if epoch % self.save_interval == 0 or epoch == self.epochs:
            save_checkpoint(self, epoch)

