import os
import shutil

from matplotlib import pyplot as plt

from model.char.config import CheckpointConfig


def clear_dir(path):
    # æ¸…ç©ºç›®å½•å‰éªŒè¯
    if os.path.exists(path):
        print(f"â™»ï¸ æ¸…ç©ºå·²æœ‰æ•°æ®ï¼š{path} (å…±{len(os.listdir(path))}ä¸ªæ—§æ ·æœ¬)")
        shutil.rmtree(path)  # åˆ é™¤æ•´ä¸ªç›®å½•æ ‘
        os.mkdir(path)  # é‡æ–°åˆ›å»ºç›®å½•
    else:
        print(f"ğŸ“ ç›®å½•ä¸å­˜åœ¨ï¼Œå·²é‡æ–°åˆ›å»º: {path}")
        os.mkdir(path)  # é‡æ–°åˆ›å»ºç›®å½•

def load_fonts(font_dir):
    """åŠ è½½å­—ä½“æ–‡ä»¶"""
    fonts = []
    for filename in os.listdir(font_dir):
        if filename.endswith('.ttf'):
            fonts.append(os.path.join(font_dir, filename))
    return fonts

def create_experiment_dir(model_name, model_params):
    """åˆ›å»ºå®éªŒç›®å½•"""
    from datetime import datetime
    params = {
        'model_name': model_name,
        'batch_size': model_params['batch_size'],
        'lr': model_params['lr'],
        'timestamp': datetime.now().strftime("%Y%m%d_%H%M%S")
    }
    dir_name = CheckpointConfig.EXPERIMENT_FORMAT.format(**params)
    exp_dir = os.path.join(CheckpointConfig.CHECKPOINT_ROOT, dir_name)
    os.makedirs(exp_dir, exist_ok=True)
    return exp_dir

def save_checkpoint(model, epoch):
    """æ£€æŸ¥ç‚¹ä¿å­˜"""
    import json
    import torch

    # ä¿å­˜æ¨¡å‹å‚æ•°
    model_name = model.name if hasattr(model, 'name') else model.__class__.__name__
    suffix = ''
    # å¦‚æœè®­ç»ƒè½®æ¬¡å°äºæ€»è½®æ¬¡ï¼Œåˆ™ä¸ºæ¨¡å‹åæ·»åŠ è½®æ¬¡ä¿¡æ¯ï¼›å¦‚æœè®­ç»ƒæ—©åœï¼Œä¸ä¼šè¿›å…¥è¯¥å‡½æ•°
    if epoch < model.epochs:
        suffix = f'_{epoch}'
    model_path = os.path.join(model.experiment_dir, f'{model_name}{suffix}.pth')
    state = {
        'model_state_dict': model.state_dict(),
        'learning_rates': model.learning_rates if hasattr(model,'learning_rates') else None,
        'epoch': epoch,
        'batch_size': model.batch_size if hasattr(model, 'batch_size') else None,
        'optimizer_state_dict': model.optimizer.state_dict() if hasattr(model, 'optimizer') else None,
        'scheduler_state_dict': model.scheduler.state_dict() if hasattr(model, 'scheduler') else None,
        'train_losses': model.train_losses if hasattr(model, 'train_losses') else None,
        'train_accs': model.train_accs if hasattr(model, 'train_accs') else None,
        'val_losses': model.val_losses if hasattr(model, 'val_losses') else None,
        'val_accs': model.val_accs if hasattr(model, 'val_accs') else None,
        'best_val_acc': f"{max(model.val_accs)*100:.2f}%" if model.val_accs else None,
        'early_stop': {
            'patience': model.early_stop_patience if hasattr(model, 'early_stop_patience') else None,
            'delta': model.early_stop_delta if hasattr(model, 'early_stop_delta') else None,
            'no_improve_counter': model.no_improve_counter if hasattr(model, 'no_improve_counter') else None
        },
        'hardware_info': {
            'device': str(model.device),
            'num_workers': model.num_workers if hasattr(model, 'num_workers') else None,
            'cuda_version': torch.version.cuda if torch.cuda.is_available() else None
        },
        'confusion_matrix': model.confusion_matrix if hasattr(model, 'confusion_matrix') else None,
    }
    torch.save(state, model_path)
    
    # ä¿å­˜è®­ç»ƒé…ç½®
    config_path = os.path.join(model.experiment_dir, 'train_config.json')
    with open(config_path, 'w') as f:
        json.dump({
            'mode_name': model.name if model.name else model.__class__.__name__,
            'learning_rates': model.learning_rates if hasattr(model,'learning_rates') else None,
            'epoch/epochs': f'{epoch}/{model.epochs}',
            'batch_size': model.batch_size if hasattr(model, 'batch_size') else None,
            'optimizer_state_dict': model.optimizer.__class__.__name__ if hasattr(model, 'optimizer') else None,
            'scheduler_state_dict': model.scheduler.__class__.__name__ if hasattr(model, 'scheduler') else None,
            'train_losses': model.train_losses if hasattr(model, 'train_losses') else None,
            'train_accs': model.train_accs if hasattr(model, 'train_accs') else None,
            'val_losses': model.val_losses if hasattr(model, 'val_losses') else None,
            'val_accs': model.val_accs if hasattr(model, 'val_accs') else None,
            'best_val_acc': f"{max(model.val_accs)*100:.2f}%" if model.val_accs else None,
            'early_stop': {
                'patience': model.early_stop_patience if hasattr(model, 'early_stop_patience') else None,
                'delta': model.early_stop_delta if hasattr(model, 'early_stop_delta') else None,
                'no_improve_counter': model.no_improve_counter if hasattr(model, 'no_improve_counter') else None
            },
            'hardware_info': {
                'device': str(model.device),
                'num_workers': model.num_workers if hasattr(model, 'num_workers') else None,
                'cuda_version': torch.version.cuda if torch.cuda.is_available() else None
            },
            'confusion_matrix': model.confusion_matrix if hasattr(model, 'confusion_matrix') else None,
        }, f, indent=2)
    
    # ä¿å­˜å­¦ä¹ æ›²çº¿
    # plot_learning_curve(
    #     train_losses=model.train_losses,
    #     val_losses=model.val_losses,
    #     val_accs=model.val_accs,
    #     save_path=os.path.join(exp_dir, 'learning_curve.png')
    # )


def plot_learning_curve(train_losses, val_losses, val_accs, save_path):
    """å¢å¼ºç‰ˆå­¦ä¹ æ›²çº¿"""
    plt.figure(figsize=(12, 6))
    
    # ä¸»Yè½´ï¼ˆæŸå¤±ï¼‰
    ax1 = plt.gca()
    ax1.plot(train_losses, 'b-', label='Train Loss')
    ax1.plot(val_losses, 'r-', label='Val Loss')
    ax1.set_xlabel('Epochs')
    ax1.set_ylabel('Loss', color='k')
    ax1.tick_params(axis='y', labelcolor='k')
    
    # æ¬¡Yè½´ï¼ˆå‡†ç¡®ç‡ï¼‰
    if val_accs:
        ax2 = ax1.twinx()
        ax2.plot(val_accs, 'g--', label='Val Acc')
        ax2.set_ylabel('Accuracy (%)', color='g')
        ax2.tick_params(axis='y', labelcolor='g')
        ax2.set_ylim(0, 100)
    
    # æ ‡æ³¨å…³é”®ä¿¡æ¯
    plt.title(f'Learning Curve (Best Val Acc: {max(val_accs)*100:.2f}%)')
    plt.grid(True)
    ax1.legend(loc='upper left')
    if val_accs:
        ax2.legend(loc='upper right')
    plt.tight_layout()
    plt.savefig(save_path, dpi=300)
    plt.close()
